{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "This notebook is just for settings the demo. \n",
    "\n",
    "It covers the following tasks:\n",
    "\n",
    "    1. Make train, test, and val directories\n",
    "    2. Split raw data in train, test and val data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "## General\n",
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import pprint\n",
    "\n",
    "# Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import apache_beam.io.iobase\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata, schema_utils, dataset_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "BASE_DIR_PATH = os.getcwd()\n",
    "DATA_DIR_PATH = os.path.join(BASE_DIR_PATH, '../data')\n",
    "RAW_DATA_PATH = os.path.join(DATA_DIR_PATH, 'raw/raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Make train, test, and val directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_directories (datapath):\n",
    "    '''\n",
    "    Given the root, create train and test folders\n",
    "    :param datapath: \n",
    "    :return: data_dir_paths\n",
    "    '''\n",
    "    train_dir = os.path.join(datapath, 'train')\n",
    "    test_dir = os.path.join(datapath, 'test')\n",
    "\n",
    "    data_dir_paths = [train_dir, test_dir]\n",
    "\n",
    "    for data_dir_path in data_dir_paths:\n",
    "        # Check if exist and remove\n",
    "        if os.path.exists(data_dir_path) and os.path.isdir(data_dir_path):\n",
    "            shutil.rmtree(data_dir_path)\n",
    "        # Make dirs\n",
    "        os.mkdir(data_dir_path)\n",
    "\n",
    "    return data_dir_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Split raw data in training, test and val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_raw_train_test (raw_data_path, data_dir_paths):\n",
    "    '''\n",
    "    Given raw data path and data directory, split the raw data\n",
    "    in train and test and store csv files\n",
    "    :param raw_data_path:\n",
    "    :param data_dir_paths:\n",
    "    :return: 1\n",
    "    '''\n",
    "    raw_df = pd.read_csv(raw_data_path)\n",
    "    train, test = train_test_split(raw_df, test_size=0.1, random_state=8)\n",
    "\n",
    "    dataframes = [train, test]\n",
    "    data_file_names = ['train.csv', 'test.csv']\n",
    "\n",
    "    for dataframe, data_dir_path, data_file_name in zip(dataframes, data_dir_paths, data_file_names):\n",
    "        dataframe.to_csv(os.path.join(data_dir_path, data_file_name), index=False)\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_paths = make_data_directories(DATA_DIR_PATH)\n",
    "split_raw_train_test(RAW_DATA_PATH, data_dir_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Task 3: Define features and configures feature columns\n",
    "\n",
    "In order to import our training data into TensorFlow, we need to specify what type of data each feature contains.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Define a dataframe for testing. \n",
    "\n",
    "# We have:\n",
    "\n",
    "# 1. missing values\n",
    "# 2. normalize data\n",
    "\n",
    "\n",
    "data = pd.DataFrame(dict(\n",
    "    a=[1.1, 2.2, 3.3, 4.4, np.NaN], # continuous\n",
    "    b=[5.5, 6.6, 7.7, np.NaN, 0.0], # continous\n",
    "    c=['a', 'b', '', 'a', ''], #categorical\n",
    "    d=[1, 2, 2, 1, 2] #target\n",
    "))\n",
    "\n",
    "data.info()\n",
    "\n",
    "data.to_csv(TEST_DATA_PATH, index=False) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- TARGET = ['d']\n",
    "CATEGORICAL_VARIABLES = ['c']\n",
    "NUMERICAL_VARIABLES = ['a', 'b']\n",
    "\n",
    "feature_columns = [] -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Set Helpers -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- def _get_mean_parameter(train_data: pd.DataFrame, column: str) -> float:\n",
    "    ''' Given a column, calculate mean'''\n",
    "    mean = train_data[column].mean()\n",
    "    return mean\n",
    "\n",
    "def _get_impute_parameters(train_data: pd.DataFrame, numerical_features: list) -> dict:\n",
    "    '''For each column in the numerical features, calculate mean.'''\n",
    "\n",
    "    impute_parameters = {}\n",
    "    \n",
    "    for column in numerical_features:\n",
    "        impute_parameters[column] = _get_mean_parameter(train_data, column)\n",
    "        \n",
    "    return impute_parameters\n",
    "\n",
    "def _impute_missing(inputs: dict) -> dict:\n",
    "    impute_parameters = _get_impute_parameters(data, NUMERICAL_VARIABLES)\n",
    "    # Since we modify just some features, \n",
    "    # we need to start by setting `outputs` to a copy of `inputs.\n",
    "    output = inputs.copy()\n",
    "    for key, value in impute_parameters.items():\n",
    "        is_miss = tf.math.is_nan(inputs[key])\n",
    "        tf_mean = tf.constant(value, dtype=np.float64)\n",
    "        output[key] = tf.where(is_miss, tf_mean, inputs[key])\n",
    "    return output\n",
    "     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Load CSV files as dataset and Impute missing -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- def input_fn(dataframe: pd.DataFrame) -> dict:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n",
    "    dataset = dataset.map(_impute_missing)\n",
    "    dataset = dataset.repeat(3).shuffle(buffer_size=5, seed=8).batch(5).prefetch(1)\n",
    "    return dataset -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Notice I use Dataset.from_tensor_slices method because data fits into memory. \n",
    "# Also it works on dictionaries, allowing this data to be easily imported.\n",
    "\n",
    "# def input_fn(dataframe: pd.DataFrame) -> dict:\n",
    "#     '''input_fn to read the data and impute missings'''\n",
    "    \n",
    "#     #Extract\n",
    "#     dataframe = _set_categorical_type(dataframe)\n",
    "#     dataframe = _set_categorical_empty(dataframe)\n",
    "#     dataframe = _set_numerical_type(dataframe)\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n",
    "    \n",
    "#     #Transform\n",
    "#     dataset = dataset.map(_impute_missing_categorical)\n",
    "#     dataset = dataset.map(_impute_missing_numerical)\n",
    "#     batch_dataset = dataset.repeat(3).shuffle(buffer_size=480, seed=8).batch(5).prefetch(1)\n",
    "    \n",
    "#     #Load : to check\n",
    "#     return dataset -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- for item in input_fn(data):\n",
    "    print(item) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # for item in input_fn(data_train).take(1):\n",
    "#     print(item) -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
